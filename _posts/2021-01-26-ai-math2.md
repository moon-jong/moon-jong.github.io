---
title: AI math2 경사하강
use_math: true
comments: true
categories : [AImath, day2]
tags : [math]
---

# Intro
머신러닝과  딥러닝의 꽃 경사하강에 대한 이야기를 하였다.
평소 경사하강에 대하여 개념적으로 잘 알고있었다 생각했지만 오늘 매운맛 경사하강을 접하며 그간의 자만을 되돌아보는 계기가 되었다.

# 경사하강
경사하강은 특정 점에서의 접선의 기울기를 구하고 그것을 이용하여 최소점을 찾아가는 방법을 이야기한다. ~~최대점을 찾는것을 경사상승법이라한다.~~

그림으로 나타내면 아래와같다.

![]({{ 'assets/img/images/GD.png' | relative_url }})

그림을 보면 한점 x에서 기울기가 양수이므로 점 x에서 기울기를 뺀다면 왼쪽으로 움직이는 것을 볼 수 있다 (x값 업데이트 -> $x = x - f(x)$)

![]({{ 'assets/img/images/GD_minus.png' | relative_url }})

위의 그림은 기울기가 음수이므로 점 x에서 기울기를 뺀다면 (-에 -를 하는 꼴이므로) x가 오른쪽으로 움직인다.


경사하강의 이러한 성질을 이용하여 오차함수의 최소값을 찾는 방식으로 학습이 진행된다.

![]({{ 'assets/img/images/gradient.png' | relative_url }})
*위의 그림은 3차원 공간에서 움푹 패인곳을 찾아가는것을 표현한 그림이다*


우리는 경사하강법을 행렬식에도 적용시키고자 한다.
따라서 각 변수별로 편미분을 계산한 그레디언트 벡터를 이용하여 적용해야한다.

![]({{ 'assets/img/images/matrix.png' | relative_url }})
*우리는 경사하강을 위와같은 행렬식에 적용시키고자 한다.*

## 그레디언트 식 유도



![]({{ 'assets/img/images/diff_1.jpg' | relative_url }})

![]({{ 'assets/img/images/diff_2.jpg' | relative_url }})

![]({{ 'assets/img/images/diff_3.jpg' | relative_url }})
