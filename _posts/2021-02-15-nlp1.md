---
title: NLP1 자연어 데이터처리
use_math: true
tags:
- NLP
comments: true
---

# Intro
자연어처리는 영상, 이미지처리와 함께 딥러닝에서 가장 발달한 분야중 하나이다.
이번주는 NLP의 전처리부터 다양한 학습방법에 대하여 기록할 예정이다.

# NLP?
> Natural language processing (NLP), which aims at properly understanding and generating human languages, emerges as a crucial application of artificial intelligence, with the advancements of deep neural networks.

NLP는 크게본다면 AI를 이용하여 인간의 언어를 해석, 이해하는 NLU, 생성하는 NLG로나눌 수 있다.

요즘 많이 사용되는 챗봇의 경우 인간의 입력을 받아서 해석하고 그에맞는 출력을 제시하는 관점에서 NLP가 적용되었다. 

이외에도 딥러닝 가능한 상태로 자연어를 변환하는 NLP(형태소, 단어, 문장, 문단단위), text에서 의미를 찾아내는 text mining, 요즘 활발하게 연구되고있는 Information retrieval(정복검색)등 자연어와 관련된 일련의 작업을 통상적으로 NLP라 부른다.

NLP는 최근 Attention을 이용한 Transformer의 등장으로 비약적인 성장을 하였다. 

대표적으로 BERT, GPT-3과 같은모델들은 전부 Transformer를 이용하여 학습되었는데 최근에 가장 눈길을 끄는것은 OpenAI에서 발표한 DALL-E이다. 

특정 문장을 입력하면 그에 맞는 이미지를 생성하는 자연어 - 이미지 생성모델이라고 이야기 할 수 있는데 이처럼 다양한 분야에서 활용성이 좋기때문에 이미지 처리와 더불어 딥러닝에서 가장 활발하게 연구되고 성장한 분야중 하나이다.


# Words embedding

딥러닝을 활용하여 자연어를 학습하기 위해선 먼저 문자를 숫자로 바꿔주어야한다. 
이 과정을 word embedding이라 부른다. 이중 Bag of words와 word2vec에 대하여 이야기하려한다.

## Bag of words
이름을 직역하여보면 단어를 담고있는 가방이다. 말 그대로 문장 혹은 문단을 단어단위로 나눈 후 그 빈도를 이용하여 key, value(단어, 빈도수)쌍으로 갖고있는 형태를 이야기한다. 

간단한 예를 들어보면 

> 내가 그린 기린 그림이 긴 기린 그림이고 네가 그린 기린 그림은 짧은 기린 그림이다.

라는 문장이 있다면 이를 우선 단어 단위로 '토큰화' 한다. 

토큰화 된 예시는 다음과 같다.

	[내, 가, 그린, 기린, 그림, 이, 긴, 기린, 그림, 이고, 네, 가, 그린, 기린, 그림, 은, 짧은, 기린, 그림, 이다, .]
	
	
그 후 단어와 빈도를 쌍으로 묶어 표현한다.

	{ .: 1, 가: 2, 그린: 2, 그린: 2, 기린: 4, 긴: 1, 내: 1, 네: 1, 은: 1, 이: 1, 이고: 1, 이다: 1, 짧은: 1}
	

~~은, 이, 이고 등은 불용어로 제거해주는것이 좋지만 여기선 우선 넘어가고...~~

그렇다면 이렇게 빈도별로 묶고 무엇을 수행할 수 있다는 것인가?

빈도를 기준으로 word cloud등 시각화도 가능하겠지만

대표적으로 Naive Bayes모델을 이용하여 특정 문장이 어떤 클래스를 갖는지 분류를 할 수 있다.


### Naive Bayes rule with bag of words
Naive Bayes rule을 간략하게 짚고 가자면

$\DeclareMathOperator*{\argmax}{arg\,max} C_{MAP} =  \underset{c \in C}{\mathrm{argmax}} P(c \mid d)$



$\DeclareMathOperator*{\argmax}{arg\,max}        = \underset{c \in C}{\mathrm{argmax}}  \frac{P(d \mid c)P(c)}{P(d)} $

$= \underset{c \in C}{\mathrm{argmax}} P(d \mid c)P(c)$

$d$는 document로 한 문장을 의미하는 것이므로 한 문장 내에서의 우도를 알아내는 것이므로 $P(d)$는 전체 동일한 값이므로 사라져도 무방하다.

$d$는 언급한것처럼 한 문장을 의미하는데 이는 단어의 집합이라 볼 수 있다. 따라서 

$P(d \mid c)P(c) = P(w_{1}, w_{2}, w_{3}, \cdots, w_{n} \mid c)P(c)$ 이다. 

하지만 모든 클래스가 주어졌을 때 모든 단어를 고려한 조건부 확률을 계산하는것은 굉장히 복잡하기때문에 단어 개별의 우도를 전부 곱하는 Naive Bayes rule을 이용한다.
따라서 식은 

$P(c) \prod_{w_{i} \in W}P(w_{i} \mid c) $ 로 계산하게된다.


이제 정말 계산해보자

![]({{ 'assets/img/images/naive_ex.png' | relative_url }})

위의 그림은 네개의 데이터를 바탕으로 마지막 문장이 NLP인지 CV인지 클래스를 맞추는 문제이다. 여기에 앞서 언급한 bag of words와 Naive Bayes모델을 이용하여 클래스를 예측한다.

우선 클래스 확률 $P(C_{cv})$ 와 $P(C_{NLP})$를 먼저 계산한다면

네개의 학습 데이터중 각 클래스가 두개씩 존재하므로 $P(C_{NLP}) = P(C_{cv}) = \frac{1}{2}$ 이다.

이 후 'classification task uses transformer'라는 문장에서 각 단어들이 각 클래스에서 몇번 등장하는지 우도들을 전부 곱한 다음 최대 우도 예측에 따라서 계산하게된다.

![]({{ 'assets/img/images/result.png' | relative_url }})

이와같은 방식으로 Bag of words와 Naive Bayes을 이용하여 문장의 클래스를 분류할 수 있다.


## word2vec
자연어를 딥러닝 모델에 학습시키기 위하여 임베딩하는 대표적인 기법이다. 
word2vec알고리즘을보면서 느낀점은 딥러닝을 시작하기도 전 전처리과정에서부터 딥?하지 않은 '러닝'의 과정이 들어간다는 점이었다.

word2vec의 중요한 가정이 있다. 
> '같은 문장에서 나타난 인접한 단어들은 비슷한  의미 혹은 특정 관계를 가질것이다 '

'인접한' 단어들이 특정한 관계를 갖는다는 가정이 있으므로 word2vec은 특정 크기의 sliding window를 이용하여 단어 쌍을 만들어내고 이를 임베딩'학습'에 이용한다.

아래 그림을 보자

![]({{ 'assets/img/images/cat_mice.jpg' | relative_url }})

그림은 sliding window의 크기를 3으로 지정하였을때 중심단어와 주변단어로 묶인 쌍을 색깔별로 표현하였다.
검은색은 'cat', 붉은색은 'hunts', 연두색은 'mice'이다. 

각 단어를 onehot인코딩한 다음에 2layer nn을 이용하여 학습시킨 후 그로부터 얻는 Weights들을 임베딩된 벡터로 사용하게된다.

![]({{ 'assets/img/images/word2vec.png' | relative_url }})

위의 그림을 보면 $W_{1}$,  $W_{2}$가 backprop되며 업데이트되는 파라미터이고 word2vec의 단어 임베딩 결과물이된다.

https://ronxin.github.io/wevi/

word2vec의 재밌는점은 단어들의 '관계'를 학습하는 것에 있다고 생각한다. 

![]({{ 'assets/img/images/king.png' | relative_url }})

'여왕' 이라는 임베딩 벡터에서 '여자'라는 벡터를 뺀 후 '남자'라는 벡터를 더하면 '왕'이라는 벡터가 나오는것을 확인할 수 있다.
(https://word2vec.kr/search/ 에서 확인 가능하다)
