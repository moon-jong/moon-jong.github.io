---
title: AI math 4 확률론 맛보기
use_math: true
tags:
- statistics
comments: true
---

# Intro
딥러닝과 머신러닝은 표본(수집한 데이터)을 바탕으로 모집단의 특징을 알아낸다는 점에서 같다.
때문에 딥러닝과 머신러닝은 통계, 확률이론을 바탕으로 만들어졌다.

그렇기에 머신러닝, 딥러닝을 공부하는 혹은 업으로 삼고있는 엔지니어, 리서처라면 통계학과 확률론의 공부가 필수적이다.
때문에 부스트캠프 AI math 4일차는 확률론에 대하여 이야기하였다.

간략히 정리하려한다.
#  확률론

## 데이터 공간

이번 확률수업은 지도학습에 사용되는 데이터공간 즉 레이블(정답)이 존재하는 공간을 가정하였다.

즉, 데이터공간을 $\mathcal{X} \times \mathcal{Y}$를 이야기하여 이를 그림으로 나타내면 아래와같이 나타난다.

![]({{ 'assets/img/images/pro_space.png' | relative_url }})

파란 점들이 $\mathcal{X} \times \mathcal{Y}$ 라는 **공간에** 분포해있는걸 묘사한 그림이다.

$\mathcal{Y}$는 1,2로 라벨이 되어있는것을 볼 수 있다.

각각의 데이터(파란점)은 **확률변수**라 부르며 $(\textbf{x} ,y)$로 표기한다



| 확률공간 |확률변수 | 확률분포|
| -------- | -------- | -------- |
| $\mathcal{X} \times \mathcal{Y}$     | $(\textbf{x} ,y)$ |$\mathcal{D}$|


## 확률 밀도함수, 질량함수?
말이 정말 어렵다. 하지만 간단하게 구별하여 쓸 수 있다.
주어진 데이터가 연속형이라면(키, 몸무게, 온도, 전압)이라면 확률 밀도함수

주어진 데이터가 이산형이라면(신발사즈, 성별, 나이[~~25.7세는 없지 않은가~~]) 확률 질량함수라 부른다.

확률 밀도함수는 연속한 공간 내에 있으므로 넓이계산, 적분이 가능하고
성별을 적분하기는 힘들지만 데이터 공간 내에 남자와 여자의 수를 세는것은 가능하므로 sum으로 나타난다.



| 확률 질량함수 | 확률 밀도함수 |
| -------- | -------- | 
| 확률 변수가 이산형일때     | 확률 변수가 연속형일때     | 
| -------- | -------- | 
|신발사이즈, 나이, 성별| 키, 몸무게, 온도, 전압|
| -------- | -------- | 
|$\sum_{y}P(\textbf{x}, y)$ |$\textstyle \int_{\mathcal{y}} P(\textbf{x}, y)dy$ |




## 조건부 분포

$P(X \vert Y = 1)$는 y가 1의 레이블을 가질 때 X의 분포이다. 조금 쉽게 이야기하자면 붉은 칸으로 나누어진 공간에서 Y = 1일때의 데이터분포를 이야기한다.

![]({{ 'assets/img/images/conditional_distribution.png' | relative_url }})

조건부 확률과 헷갈리기 좋은데

조건부 확률은 정답 레이블이 1일 때 입력변수 x일 확률을 뜻한다 따라서 표기도 $P(Y = 1 \vert X)$이다.



| 조건부 분포 | 조건부 확률 |
| -------- | -------- | 
| 정답레이블(Y = 1일때)의 확률 변수들의 **분포**     | 확률변수 분포에서 정답이 1일 **확률**   | 
| -------- | -------- | 
|$P(X \vert Y = 1)$ | $P(Y = 1 \vert X)$ |


## 기대값
한줄정리 : 데이터를 대표하는 통계량이면서 동시에 확률 분포를 통해 다른 통계적 범함수를 계산하는데 사용되는 값
수식
- 확률변수가 연속형일때 : $ \textstyle \int_{\mathcal{\chi}}f(x)P(\textbf{x}, y)dx $
- 확률변수가 이산형일때 : $\sum_{\chi}f(x)P(\textbf{x}, y)$

말이 너무 어렵다. 

기대값은 고등학교 미통기 시간에 '평균'으로 퉁쳐서 배운 개념이었다.
하지만 '평균'이라는 개념은 단지 어떠한 확률분포에서 각각의 데이터값이 뽑힐 확률이 전부 동일할 때 기댓값은 평균이라 할 수 있다.
주사위던지기같은 경우가 그렇다 

이산 확률변수를 갖는 데이터 공간을 보자(주사위놀이)

위의 수식에 따르면 주사위 던지기의 기댓값은

$ 1 * \frac{1}{6} + 2 * \frac{1}{6} + 3 * \frac{1}{6} + 4 * \frac{1}{6} + 5 * \frac{1}{6} + 6 * \frac{1}{6} = \frac{1}{6} * (1 + 2 + 3 + 4 + 5 + 6 ) = 3.5 $이므로 평균값이다.

하지만 이는 주사위를 던졌을 때 6면 각각 나올 확률이 같다고 가정하였을 때이다. 

만약 6이 나올 확률이 더 높게 설정된 주사위라면 결과값은 평균이 아닌 다른값으로 나올것이다.

## 몬테카를로 샘플링

몬테카를로 샘플링은 확률분포를 명시적으로 알 수 없을 때 충분히 큰 샘플을 수집하여 (독립추출) 데이터의 분포를 가정하는 방법이다.

기계학습에서 매우 다양하게 응용되는 방법!!

몬테카를로 샘플링을 이용하여 $\pi$를 구하는 방법은 굉장히 유명하다 ([다트던지기](https://youtu.be/5cNnf_7e92Q))

하지만 샘플링이 충분히 크다는 가정이 있다!!  
위의 다트던지기에서 다트를 100개 던졌을 때, 1000개 던졌을 때, 1만개 던졌을 때 오차가 달라지는것을 확인할 수 있다.

당연한 이야기지만 1만개 던졌을 때 실제 $\pi$값과 가장 작은 오차를 보인다.
(~~[몬테카를로를 이용한 적분](https://youtu.be/WAf0rqwAvgg) 도 해보길 바란다~~)


# Outro
정말 용어와 수식표현을 정리하느라 정신없었다.
이론적으로 굉장히 어려운 부분은 들어가지 않았기 때문에(~~맛보기라!!~~) 이론적으로 이해하기 보다는 수식의 기호들이 의미하는 것이 무엇인지 알아보는 과정이 힘들었다.
내일은 또 어떤 수학이 기다리고있을지...
