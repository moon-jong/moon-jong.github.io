---
title: CV 시간과 데이터가 부족할 때
use_math: true
tags:
- ComputerVision
---

# Intro
GPU성능의 향상, 모델링 기법의 발전 등 딥러닝이 발전할 수 있었던 이유는 많지만 사실 가장 큰 이유는 수많은 데이터의 축적이다.

[](http://img.lb.inews24.com/image_joy/201212/1355708207737_1_103913.jpg)

*출처:http://www.inews24.com/view/712092*

데이터는 지금도 그 종류와 속도, 양 측면에서 기하급수적으로 증가하고있다.
머신러닝은 기본적으로 데이터를 기반으로 표본을 바탕으로 모집단의 특징을 알아내는 작업이다.

데이터가 많으면 많을수록 모집단을 더 잘 대표할 수 있기때문에 데이터의 양이 딥러닝의 성능에 절대적인 역할을 한다.

하지만 딥러닝을 하기위한 데이터가 잘 마련이 되어있다면 상관이 없겠지만 대부분은 그 양이 충분치 않거나 퀄리티가 떨어지는 이슈가 있다.
따라서 갖고있는 데이터를 잘 조작하거나 비슷한 태스크를 수행하는 다른 잘 훈련된 모델을 가져다 쓰는 방식으로 학습을 진행한다.

# Pretrained model
한번쯤은 누가 나 대신 공부해주었으면 하는 생각을 해보았을것이다. 
인간의 경우 그게 불가능하겠지만 딥러닝 모델의 경우 약간 이야기가 달라진다.

지금은 수많은 데이터를 미리 학습하고 파라미터가 어느정도 튜닝된 모델들이 존재한다. 대표적으로 GPT-3, BERT등이 될것이다.
이러한 모델들은 이미 여러곳에서 뛰어난 성능을 뽐낸다고 이미 알려져있다.
때문에 우리는 이러한 똑똑한 모델들을 우리가 직접 구현하지 않고 가져다 사용하는 방식도 채용할 수 있다. (~~GPT-3는 불가능...~~)


## Transfer learning
앞서 언급한것처럼 우리에게 주어진 시간과 데이터가 많지 않다면 직접 모델링을 구현하는 것 보다 비슷한 역할을 하는 똑똑한 모델을 가져다 사용하는것이 가능하다.

![]({{ 'assets/img/images/transferLearning.png' | relative_url }})

Transfer learning(이하 전이학습)은 위와같은 구조를 갖는다.
사진을 자세히 보면 kernel을 이용하여 학습하는 Conv 레이어는 그대로 두고 Fully connected layer를 새로 초기화하여 내가 수행하려는 테스크에 맞게 fine tuning을 진행한다.
이미 기존에 cifar100같은 데이터를 이용하여 잘 학습된 Lenet, VGGnet등 기존의 유명한 모델을 붙인 뒤 파라미터를 freeze함으로써 기존의 학습된 부분을 고정시키고
 Fully connected layer만을 학습시켜 우리가 원하는 테스크를 수행하게 한다.
 
 만약 데이터가 조금 더 많다면 기존 모델의 Conv 레이어에 매우 작은 learning rate을 곱하여 학습하는것도 가능하다.
 
## Knowledge distillation
 
 만약 확보할 수 있는 데이터의 양은 많지만 Labeling이 되어있지 않다면?
 Labeling은 시간적으로나 비용적으로나 굉장한 소모가 되는 작업이다.
 그리고 무엇보다 필수적이다.
 
하지만 수천만 ~ 수억개의 데이터를 사람이 일일히 Labeling을 한다는 것은 어불 성설이다.
때문에 Knowledge distillation을 이용하여 이를 유사 라벨링하여 학습데이터로 사용하는 방법이 제시되었다.

이는 모델 경량화에서 먼저 나온 이야기이다. 굉장히 많은 수의 파라미터를 갖고있고 많은 학습데이터를 학습한 모델이라면 엣지 디바이스, 유저단에서 사용이 불가능하거나 굉장히 느린 속도로 동작할 수 있다.
때문에 좋은 Teacher model을 만들고 이를 적은 수의 파라미터를 갖고있는 Student model이 Teacher모델의 output distribution을 모사하도록 학습시켜 모델의 사이즈를 줄인다.

그러나 이 기법이 데이터의 라벨링을 자동화하는 용도로 사용된다.

![]({{ 'assets/img/images/distillation.png' | relative_url }})

위 그림의 전체적인 프로세스는 다음과 같다.

1. 먼저 Teacher 모델을 만들고 이를 갖고있는 데이터를 통하여 학습시킨다.
2. Teacher모델에게 Labeling이 되어있지 않은 데이터를 주입하고 Prediction을 Label로 이용한다.
3. 기존의 데이터와 Teacher모델이 예측한 데이터를 이용하여 Student 모델을 학습시킨다.
4. 학습된 Student모델을 Teacher model로 사용하여 1,2,3을 반복한다.

위와같은 방법을 Noisy Student Traning이라 부르며 압도적인 성능을 증명한 바 있다.

![]({{ 'assets/img/images/noisy_student_training.png' | relative_url }})



# Data Augmentation
좋은 성능의 모델을 가져다 사용하는것도 방법이지만 절대적인 데이터의 양을 늘리는 방법도 존재한다.
이는 이미지 데이터의 장점중 하나이기도 하다.

우리는 같은 강아지 이미지라도 옆으로 뒤집거나 조금 자르거나 명암을 다르게 하거나 하더라도 알아볼 수 있다.
하지만 컴퓨터에게 학습시키는 강아지 이미지는 대부분 사람들이 구도를 생각하고 찍은 것이라 노이즈가 섞인 데이터는 거의 없다.
다시말해 사람 손에 의해 Bias되었다 이야기 할 수 있고 이는 전체 세상의 강아지를 커버하는것이 아닌 굉장히 작은 부분을 묘사하고 있을것이다.

![]({{ 'assets/img/images/sample_vs_real.png' | relative_url }})

이를 어느정도 완화하기위해 Data augmentation이라는 기법을 사용한다.

![]({{ 'assets/img/images/augmentation.png' | relative_url }})

같은 이미지를 자르고 뒤집고 명암 반전을 주는 등 조작을 이용하여 마치 다른이미지처럼 만들어 학습을 시키는 것이다.

한가지 문제가 있다. 
어떤 Augmentation을 수행해야 모델이 가장 잘 학습할 수 있을까?
자르고 뒤집고 반전? 
반전 뒤집고 자르기?

![]({{ 'assets/img/images/augpolicy.png' | relative_url }})

방법이 많은만큼 경우의 수도 무수히 많다. 때문에 적절한 Augmentation기법을 찾는 RandAug가 제안되었다.
적용하고자 하는 Augmentation을 입력하고 이를 grid search등으로 최적의 Augmentation을 찾는것이다.

# outro
CV의 장점중 하나는 augmentation을 이용하여 데이터를 양을 늘릴 수 있다는 것이라 생각한다.
자연어, 시계열 데이터와 다르게 어느정도 갖고 있는 데이터를 조작하여 모집단을 조금이라도 더 잘 대표하는 데이터를 생성해 내는것은 딥러닝 부분에서 CV의 독자적인 발전을 이루어낼 수 있도록 해준 원동력이 아닌가 싶다.
