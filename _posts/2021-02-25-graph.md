---
title: Graph 정점을 벡터로?
use_math: true
tags:
- graph
---

# Intro
그래프를 기계학습에 어떻게 이용할까?라는 의문은 그래프를 어떻게 수의 집합으로 표현할까?와 비슷한 맥락이다.
만약 그래프를 벡터로 표현할 수 있다면 MLP, k-means clustering 등 다양한 알고리즘을 통하여 정점분류 혹은 군집분석 등에 사용할 수 있을것이다.

하지만 정점과 간선으로 되어있는 그래프를 어떻게 벡터로 표현할 수 있을까?


# 정점 표현 학습(Node Embedding)
정점 표현 학습은 지난 NLP에서 다루었던 워드 임베딩과 비슷한 면을 갖고있다.
워드 임베딩 역시 문장에서 단어들의 관계를 학습을 통해 특정 단어에 대한 임베딩 벡터를 얻어낸것과 마찬가지로 정점 표현 학습 역시 그래프의 정점을 특정 임베딩 벡터를 통하여 임베딩 공간으로 옮기는 것을 이야기한다.

![]({{ 'assets/img/images/embedding.png' | relative_url }})

따라서 하나의 정점은 하나의 벡터 표현을 갖게되고 이를 임베딩한다 라고 이야기한다.

하지만 중요한것은 어떻게 **정점간 유사도를 임베딩 공간에서도 보존할까??** 이다.

![]({{ 'assets/img/images/embedding_space.png' | relative_url }})

## 유사도
정점간 유사도는 두 정점의 내적으로 계산된다. 
따라서 $u, v$ 두 정점이 서로 그래프상에서 인접하였다면 임베딩 공간에서 두 벡터$u, v$역시 높은 유사도를 지니고 있어야 한다.

하지만 내적을 통한 유사도 계산은 임베딩공간에 벡터로 정의된 정점끼리는 가능하지만 그래프에서의 유사도는 어떻게 계산해야할까?

이 질문에 대한 세가지의 접근방식을 소개하려고 한다.

## 인접성 기반의 접근법
인접성 기반 접근법은 두 정점이 인접할때 유사하다 판단한다.
인접하다는 이야기는 두 정점이 간선으로 연결되어있음을 의미한다.

때문에 그래프에 대한 인접행렬을 구한 뒤 두 정점이 인접한경우 1, 아닌경우 0으로 하여 유사도를 가정한다.

![]({{ 'assets/img/images/AdjacencyMatrix.png' | relative_url }})

위의 그래프에서 정점 6과 4는 연결되어있으므로 인접하다. 따라서 유사하다고 이야기 할 수 있지만 정점 6과 5는 간선으로 연결되어있지 않기때문에 유사도가 낮다.
따라서 인접행렬의 원소를 바탕으로 유사도를 판단한다.

이를 바탕으로 손실함수를 만들게되는데

$L = \sum \lVert z_{u} \cdot z_{v} - A_{u,v} \rVert^2$로 정의된다. $A_{u,v}$는 인접행렬에서 정점 $u$행 $v$열로 정점 $u$와 $v$의 간선유무를 표현하는 원소다.
따라서 임베딩 공산에서 유사도가 그래프에서의 유사도와 차이가 나지 않는다면 손실이 적고 차이가 크다면 손실함수 역시 커지게된다.

이를 최적화를 이용하여 줄이는 방향으로 임베딩이 진행된다.

하지만 인접성 기반의 접근법은 극명한 한계가 있다.
![]({{ 'assets/img/images/nodes.jpeg' | relative_url }})

위의 그림에서 보면 붉은색 1번 정점과 5번 정점은 거리가 3, 8번 정점과 5번 정점은 거리가 2이다. 
하지만 유사도는 모두 0으로 동일하다.
인접성 기반의 유사도 측정은 정점간의 거리를 무시한다.

또한 1번 정점은 {1,2,3}군집에 속해있고 5번 정점과 8번 정점은 {5,6,7,8} 군집에 속해있다고 추정할 수 있다.
같은 군집에 속해있음에도 불구하고 두 정점의 유사도는 0으로 동일하다.

인접성 기반의 접근법은 두 정점의 거리, 군집 등을 무시하고 연결성만을 고려한다는 한계가 있다.

이러한 한계를 보완하고자 다른 접근법들이 등장하였다.

## 거리/경로/중첩 기반 접근법

### 거리기반 접근법
거리 기반 접근법은 두 정점 사이의 거리가 *충분히* 가까운 경우 두 정점은 유사하다 판단한다.
'충분히'라는 정의는 설계자의 하이퍼 파라미터로 **정하기 나름이다**

![]({{ 'assets/img/images/distance_approch.png' | relative_url }})

두 정점의 거리가 2이하라면 충분히 가깝다고 가정해보자
붉은색 정점과 초록색 정점들, 푸른색 정점들은 거리가 2이내이므로 충분히 가깝다. 따라서 '유사하다'라 판단할 수 있다

하지만 붉은색 정점과 보라색 정점들은 거리가 3이므로 유사하다 판단하지 않는다.

### 경로기반 접근법
경로 기반 접근법은 두 정점 사이의 경로가 많을수록 유사하다 간주한다.
경로는 지난 graph basic에서 정의한바와 같이 

1. u에서 시작해서 v로 끝나야하며
2. 순열에서 연속된 정점은 간선으로 연결되어있어야 한다.

![]({{ 'assets/img/images/big_connection_factor.png' | relative_url }})

위의 그래프에서 1 -> 6 -> 8과 같은 순열은 경로가 될 수 없다.
또한 경로는 k로 제한되기때문에 2부터 4까지의 경로에 
2 -> 3 -> 1 -> 3 -> 1 $\cdots$ ->4와같은 경로 역시 k의 범위를 초과하면 포함될 수 없다.

손실함수 역시 조금 달라지게 되는데 
$L = \sum \lVert z_{u} \cdot z_{v} - A_{u,v}^k \rVert^2$ 로 k는 인접행렬 $A$의 k제곱의 $u$행 $v$열의 원소이다.

### 중첩기반 접근법
중첩기반 접근법은 각 정점이 몇개의 같은 이웃을 갖고있는지 찾아내게된다.

![]({{ 'assets/img/images/jung_chup.png' | relative_url }})

가령 위의그림에서 붉은 정점과 파란 정점은 서로 두개의 공통이웃을 갖고있는데 이 때에 붉은 정점과 파란 정점은 중첩기반 접근법으로 유사하다 판단한다.

따라서 손실함수 역시 
$L =  \sum \lVert z_{u} \cdot z_{v} - S_{u,v} \rVert^2$로 정의되는데 $ S_{u,v}$는 정점 u와 v가 공통으로 갖고있는 정점의 수이다.

하지만 위의 손실함수는 공유하고있는 정점의 수가 굉장히 많다면 값이 매우 커질 수 있기때문에 공통의 이웃 수 $S_{u,v}$대신 **자카드 유사도** 혹은 **Adamic Adar점수**를 사용한다.

자카드 유사도는 공통의 이웃 수 대신 두 정점 $u,v$의 전체 이웃수에서 공통으로 몇개의 이웃을 갖고있는지 **비율**을 나타낸다. 

Adamic Adar점수는 공통의 이웃 각각에 가중치를 부여하여 가중합을 계산하는 방식이다.

$Adamic Adar = \sum frac{1}{d_{w}}$
